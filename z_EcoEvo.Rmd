---
title: "Untitled"
output: html_document
date: '2023-04-17'
---


## Setup

```{r setup, echo = FALSE}
# Tidy
 # rm(list=ls())
 # graphics.off()
# Preparing workspace
knitr::opts_chunk$set(echo = TRUE, include = TRUE)
# Loading packages
pacman::p_load(knitr, # knit markdown
               readxl, 
               readr, 
               dplyr, 
               tidyverse,
               janitor, 
               patchwork, # layout of plots
               cowplot, 
               ggpubr,
               metafor,
               gridExtra,
               #orchaRd, # forest-like plot
               gridGraphics, # Redraw Base Graphics Using 'grid' Graphics. `gridGraphics` is required to handle base-R plots.
               #dabestr,
               here,
               lme4,
               car, # logit transformation, car::logit()
               #boot, # Bootstrap Resampling
               lmerTest,
               #vcd,
               ggthemes,
               RColorBrewer,
               raincloudplots,
               wesanderson,
               ggsci, # palette: https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/
               flexmix,
               effects,
               grDevices
               )

```

# Help functions

```{r custom functions}
#source(here("custom_func","custom_func.R"))

# write a series help functions
dmix <- function(x,p,mean,sd) {
  p %*% sapply(x, function(x) dnorm(x, mean = mean, sd = sd))
}
pmix <- function(x,p,mean,sd) {
  drop(p %*% sapply(x, function(x) pnorm(x, mean = mean, sd = sd)))
} 
rmix <- function(n,p,mean,sd) {
  d = rmultinom(n,1,p)
  rnorm(n,mean%*%d,sd%*%d)
}



#qmix(P = 0.05,p,mu,tau)
#pmix(x = 0.5125133,p,mu,tau)


posterior_bhat <- function(b,s,p,tau) {
  z = b / s
  tau2 <- tau^2
  q <- p * dnorm(z, 0, sqrt(tau2 + 1))
  q <- q / sum(q)
  pm <- (b * tau2) / (tau2 + 1)
  pv <- (s^2 * tau2) / (tau2 + 1)
  ps <- sqrt(pv)
  #data.frame(q,pm,pv,ps)
  b_hat = sum(q * pm)
  #lamda = b/b_hat
  b_hat
}

posterior_snr <- function(b,s,p,tau) {
  z = b / s
  tau2 <- tau^2
  q <- p * dnorm(z, 0, sqrt(tau2 + 1))
  q <- q / sum(q)
  pm <- (z * tau2) / (tau2 + 1)
  pv <- (tau2) / (tau2 + 1)
  ps <- sqrt(pv)
  data.frame(q,pm,ps)

}

posterior <- function(b,s,p,tau) {
  z = b / s
  tau2 <- tau^2
  q <- p * dnorm(z, 0, sqrt(tau2 + 1))
  q <- q / sum(q)
  pm <- (b * tau2) / (tau2 + 1)
  pv <- (s^2 * tau2) / (tau2 + 1)
  ps <- sqrt(pv)
  data.frame(q,pm,pv,ps)
  #b_hat = sum(q * pm)
  #lamda = b/b_hat
  #data.frame(b_hat = b_hat)
}

shrink <- function(b, s, p, tau) {
  post = posterior(b, s, p, tau)
  b/sum(post$q * post$pm)
}


# custom functions for visualization
manhattan <- function (x, chr = "CHR", bp = "BP", p = "P", snp = "SNP", col = c("gray10", 
  "gray60"), chrlabs = NULL, suggestiveline = -log10(1e-05), 
  genomewideline = -log10(5e-08), highlight = NULL, logp = TRUE, 
  annotatePval = NULL, annotateTop = TRUE, ...) 
{
  CHR = BP = P = index = NULL
  if (!(chr %in% names(x))) 
    stop(paste("Column", chr, "not found!"))
  if (!(bp %in% names(x))) 
    stop(paste("Column", bp, "not found!"))
  if (!(p %in% names(x))) 
    stop(paste("Column", p, "not found!"))
  if (!(snp %in% names(x))) 
    warning(paste("No SNP column found. OK unless you're trying to highlight."))
  if (!is.numeric(x[[chr]])) 
    stop(paste(chr, "column should be numeric. Do you have 'X', 'Y', 'MT', etc? If so change to numbers and try again."))
  if (!is.numeric(x[[bp]])) 
    stop(paste(bp, "column should be numeric."))
  if (!is.numeric(x[[p]])) 
    stop(paste(p, "column should be numeric."))
  if (!is.null(x[[snp]])) 
    d = data.frame(CHR = x[[chr]], BP = x[[bp]], P = x[[p]], 
      pos = NA, index = NA, SNP = x[[snp]], stringsAsFactors = FALSE)
  else d = data.frame(CHR = x[[chr]], BP = x[[bp]], P = x[[p]], 
    pos = NA, index = NA)
  d <- d[order(d$CHR, d$BP), ]
  if (logp) {
    d$logp <- -log10(d$P)
  }
  else {
    d$logp <- d$P
  }
  d$index = rep.int(seq_along(unique(d$CHR)), times = tapply(d$SNP, 
    d$CHR, length))
  nchr = length(unique(d$CHR))
  if (nchr == 1) {
    d$pos = d$BP
    xlabel = paste("Chromosome", unique(d$CHR), "position")
  }
  else {
    lastbase = 0
    ticks = NULL
    for (i in unique(d$index)) {
      if (i == 1) {
        d[d$index == i, ]$pos = d[d$index == i, ]$BP
      }
      else {
        lastbase = lastbase + max(d[d$index == (i - 
          1), "BP"])
        d[d$index == i, "BP"] = d[d$index == i, "BP"] - 
          min(d[d$index == i, "BP"]) + 1
        d[d$index == i, "pos"] = d[d$index == i, "BP"] + 
          lastbase
      }
    }
    ticks <- tapply(d$pos, d$index, quantile, probs = 0.5)
    xlabel = "Chromosome"
    #labs <- unique(d$CHR)
    labs <- c("SMD", "lnRR", "Zr", "uncommon")
  }
  xmax = ceiling(max(d$pos) * 1.03)
  xmin = floor(max(d$pos) * -0.03)
  def_args <- list(xaxt = "n", bty = "n", xaxs = "i", yaxs = "i", 
    las = 1, pch = 20, xlim = c(xmin, xmax), ylim = c(0, 
      ceiling(max(d$logp))), xlab = xlabel, ylab = expression(-log[10](italic(p))))
  dotargs <- list(...)
  do.call("plot", c(NA, dotargs, def_args[!names(def_args) %in% 
    names(dotargs)]))
  if (!is.null(chrlabs)) {
    if (is.character(chrlabs)) {
      if (length(chrlabs) == length(labs)) {
        labs <- chrlabs
      }
      else {
        warning("You're trying to specify chromosome labels but the number of labels != number of chromosomes.")
      }
    }
    else {
      warning("If you're trying to specify chromosome labels, chrlabs must be a character vector")
    }
  }
  if (nchr == 1) {
    axis(1, ...)
  }
  else {
    axis(1, at = ticks, labels = labs, ...)
  }
  col = rep_len(col, max(d$index))
  if (nchr == 1) {
    with(d, points(pos, logp, pch = 20, col = col[1], ...))
  }
  else {
    icol = 1
    for (i in unique(d$index)) {
      points(d[d$index == i, "pos"], d[d$index == i, "logp"], 
        col = col[icol], pch = 20, ...)
      icol = icol + 1
    }
  }
  if (suggestiveline) 
    #abline(h = suggestiveline, col = "blue")
  if (genomewideline) 
    #abline(h = genomewideline, col = "red")
  if (!is.null(highlight)) {
    if (any(!(highlight %in% d$SNP))) 
      warning("You're trying to highlight SNPs that don't exist in your results.")
    d.highlight = d[which(d$SNP %in% highlight), ]
    with(d.highlight, points(pos, logp, col = "green3", 
      pch = 20, ...))
  }
  if (!is.null(annotatePval)) {
    if (logp) {
      topHits = subset(d, P <= annotatePval)
    }
    else topHits = subset(d, P >= annotatePval)
    par(xpd = TRUE)
    if (annotateTop == FALSE) {
      if (logp) {
        with(subset(d, P <= annotatePval), textxy(pos, 
          -log10(P), offset = 0.625, labs = topHits$SNP, 
          cex = 0.45), ...)
      }
      else with(subset(d, P >= annotatePval), textxy(pos, 
        P, offset = 0.625, labs = topHits$SNP, cex = 0.45), 
        ...)
    }
    else {
      topHits <- topHits[order(topHits$P), ]
      topSNPs <- NULL
      for (i in unique(topHits$CHR)) {
        chrSNPs <- topHits[topHits$CHR == i, ]
        topSNPs <- rbind(topSNPs, chrSNPs[1, ])
      }
      if (logp) {
        textxy(topSNPs$pos, -log10(topSNPs$P), offset = 0.625, 
          labs = topSNPs$SNP, cex = 0.5, ...)
      }
      else textxy(topSNPs$pos, topSNPs$P, offset = 0.625, 
        labs = topSNPs$SNP, cex = 0.5, ...)
    }
  }
  par(xpd = FALSE)
}


```


# Preprocess

Import the data (the .csv file named 'all effect size data edited 24-08')

```{r}
dat_all <- read.csv(here("data","all effect size data edited 24-08.csv")) 
# check data
head(dat_all)
dim(dat_all)

# basic cleaning 
dat_all <- dat_all[!is.na(dat_all$eff.size) & !is.na(dat_all$var.eff.size), ] # remove NAs
dat_all <- dat_all[dat_all$var.eff.size != 0, ] # remove negative and zero variance (sampling variance should be positive)
# classify effect size measures
grouped_es <- NA
grouped_es[dat_all$eff.size.measure == "cohens.d"|
           dat_all$eff.size.measure == "hedges.d"|
           dat_all$eff.size.measure == "hedges.g"|
           dat_all$eff.size.measure == "abs.hedges.d"|
           dat_all$eff.size.measure == "SMD"] <- c("SMD") # (1) SMD: cohens.d,  hedges.d, hedges.g, abs.hedges.d, SMD
grouped_es[dat_all$eff.size.measure == "log.ratio"] <- c("lnRR") # (2) lnRR: log.ratio
grouped_es[dat_all$eff.size.measure == "z.r"] <- c("Zr") # (3) Zr: z.r
grouped_es[dat_all$eff.size.measure == "mean.diff" |
           dat_all$eff.size.measure == "reg.slope" | 
           dat_all$eff.size.measure == "IRR" | 
           dat_all$eff.size.measure == "log.odds.ratio"] <- c("uncommon") # (5) uncommon: mean.diff, reg.slope
dat_all$grouped_es <- as.factor(grouped_es)



#-----------------------not run-------------------------------
#dat_list <- readRDS(here("data","dat_list.Rds"))

#dat_esc <- NA
#for (i in 1:length(dat_list)) {
#  dat_esc[i] <- escalc(yi = eff.size, vi = var.eff.size, data = dat_list[[i]]) %>% list()
#}

#dat_agg <- NA
#for (i in 1:length(dat_list)) {
#  dat_agg[i] <- aggregate.escalc(dat_esc[[i]], cluster = study, rho = 0.5) %>% #list()
#}

#saveRDS(dat_agg, file = "dat_agg.Rds")
#readRDS(here("data","dat_agg.Rds"))
#dat_agg2 <- plyr::ldply(dat_agg, data.frame) # https://stackoverflow.com/questions/2851327/combine-a-list-of-data-frames-into-one-data-frame-by-row
# further cleaning
#dat <- dat_agg2 %>% select(meta.analysis.paper, meta.analysis.year, study, study.year, eff.size.measure, grouped_es, eff.size, var.eff.size)
#-----------------------not run-------------------------------



# further cleaning
dat <- dat_all %>% select(meta.analysis.id, meta.analysis.year, study, study.year, eff.size.measure, grouped_es, eff.size, var.eff.size) # only select useful variables:

#--------------explanation of each variable--------------e#
# meta.analysis.id - identity for meta-analysis included in our dataset

# meta.analysis.year - the publication year of the meta-analysis

# study - unique identity of each primary studies within each meta-analysis

# study.year - publication year of the primary studies within a meta-analysis, a predictor of replication success see Figure 1D

# eff.size.measure - the type of effect size used in the meta-analysis

# eff.size.measure - the broad type of effect size used in the meta-analysis,  a predictor of replication success see Figure 1E

# eff.size - effect size estimate

# var.eff.size - sampling variance of effect size estimate; the square root is standard error of effect size


dat <- dat %>% mutate(se.eff.size = sqrt(var.eff.size)) # create the variable of sampling error
dat <- dat %>% mutate(z = eff.size/se.eff.size) # calculate z value
dat <- dat %>% filter(z < 40 & z > -40) # delete potential outliers
#dat <- filter(dat,grouped_es != "uncommon") # only focus on common measures
#z <- abs(dat$z)
z <- dat$z # using the original z value rather than absolute version

```


# Mixture model

```{r}
set.seed(2023)
#fit <- readRDS(file = "fit_fitted.Rds")
fit <- flexmix(z ~ 0, k = 3, data = dat, model = FLXMRglm(family = "gaussian")) # non-zero distribution with 4 components led to less than 1 sigma
p <- summary(fit)@comptab$prior
#m <- modeltools::parameters(fit)[1,] # non-zero distribution
#sigma <- modeltools::parameters(fit)[2,]
#ind <- order(sigma)
mu <- rep(0, length(p)) # I deliberately kept mu here, although we set mean 0 when fitting mixture model. The reason is that in case we extend it to non-zero mean mixture model
sigma <- modeltools::parameters(fit)
ind <- order(sigma)
p <- p[ind]
mu <- mu[ind]
sigma <- sigma[ind]
tau <- sqrt(sigma^2 - 1)
#saveRDS(fit, file = "fit_fitted.Rds")

```

# Fig 1A - z value distribution and fitted line

Distribution of z and its fitted mixture model

```{r}
x <- seq(-10,10,0.01)
n <- length(z)
dist <- data.frame(x, y = drop(dmix(x,p,mu,sigma)))
#df <- data.frame(z = z)
df <- data.frame(z = c(-z, z))


Fig_1A <- df %>% 
  ggplot(aes(x = z)) + 
  geom_histogram(aes(y = ..density..), 
                 #breaks = breaks, 
                 fill = brewer.pal(n = 8, name = "Dark2")[1], 
                 color = brewer.pal(n = 8, name = "Dark2")[8],
                 alpha = .5,
                 bins = 50) +
  theme_cowplot() + 
  labs(x = "Test statsitic of signal experiment (z value)", y = "Density") + 
  scale_fill_brewer(palette="Dark2") +
  xlim(-10,10) +
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01), 
                     limits = c(0,0.27), breaks = seq(0,0.25,0.05)
                     ) + # need "scales" package
  #scale_y_continuous(limits = c(0,0.2), breaks = seq(0,0.2,0.05)) + 
  geom_line(data = dist, aes(x = x, y = y), color = brewer.pal(n = 8, name = "Dark2")[2], alpha=1, linewidth = 1.5) + 
  theme(axis.title = element_text(size = 14, colour = "black"),
        axis.text = element_text(size = 14, colour = "black"))
Fig_1A 
```


# Fig 1B - SNR

Distribution of SNR

```{r}
set.seed(2023)
x <- seq(-10,10,0.01)
dist2 <- data.frame(x, y = drop(dmix(x,p,mu,tau))) 


#  calculate quantiles - needs to ask Erik to help write quantile function for the mixture distribution
Q25 <- quantile(rmix(length(x),p,mu,tau),0.25)
Q75 <- quantile(rmix(length(x),p,mu,tau),0.75)
Q50 <- quantile(rmix(length(x),p,mu,tau),0.50)
# https://stackoverflow.com/questions/48212824/shaded-area-under-density-curve-in-ggplot2

Fig_1B <- dist2 %>% 
  ggplot(aes(x = x, y = y)) + 
  geom_vline(xintercept = Q25, linetype=2, color = brewer.pal(n = 8, name = "Dark2")[4]) + 
  geom_vline(xintercept = Q75, linetype=2, color = brewer.pal(n = 8, name = "Dark2")[4]) + 
  geom_segment(aes(x = Q25-4, y=0.26, xend=Q25, yend=0.26), 
               arrow = arrow(length=unit(0.2, 'cm'), 
                             ends="last", # first, last, both # https://stackoverflow.com/questions/54846715/adding-arrows-into-ggplot
                             type = "closed"),
               color='lightgrey', lwd=0.2) + 
  annotate("text", x = Q25-2, y = 0.267, label = "Q25", color = brewer.pal(n = 8, name = "Dark2")[4]) + 
  geom_segment(aes(x = Q75, y=0.26, xend=Q75+4, yend=0.26), 
               arrow = arrow(length=unit(0.2, 'cm'), 
                             ends="first", 
                             type = "closed"),
               color='lightgrey', lwd=0.2) + 
  annotate("text", x = Q75+2, y = 0.267, label = "Q75", color = brewer.pal(n = 8, name = "Dark2")[4]) + 
  geom_ribbon(data = subset(dist2, x < Q25), aes(x = x, ymax = y), ymin = 0, fill = brewer.pal(n = 8, name = "Dark2")[5], alpha = 0.2) + # https://stackoverflow.com/questions/20355849/ggplot2-shade-area-under-density-curve-by-group
  geom_ribbon(data = subset(dist2, x > Q75), aes(x = x, ymax = y), ymin = 0, fill = brewer.pal(n = 8, name = "Dark2")[5], alpha = 0.2) +
  #geom_line(color = brewer.pal(n = 8, name = "Dark2")[8], linewidth = 1) +
  geom_ribbon(data = subset(dist2, x >= Q25 & x <= Q75), aes(x = x, ymax = y), ymin = 0, fill = brewer.pal(n = 8, name = "Dark2")[4], alpha = 0.2) +
  #geom_area(fill = "lightgrey",alpha = 0.5) + 
  theme_cowplot() + 
  labs(x = "Signal-noise-ratio of signal experiment (SNR)", y = "Density") + 
  xlim(-10,10) + 
  scale_y_continuous(labels = scales::number_format(accuracy = 0.01), 
                     limits = c(0,0.27), breaks = seq(0,0.25,0.05)
                     ) + # need "scales" package
  theme(axis.title =  element_text(size = 14, colour = "black"),
        axis.text =  element_text(size = 14, colour = "black"))
Fig_1B
```

# Fig 1C - replication vs. SNR

The association between replication success and SNR 

```{r}
# simulate the same number of replicates as the number of observations in dataset
set.seed(2023)
snr <- rmix(n, p, mu, tau)

# calculate average success rate of typical replication, which is defined as statistically significant and the same direction
z.repl <- snr + rnorm(n)
repl <- (z * z.repl > 0 ) & (abs(z.repl) > qnorm(1 - 0.05/2)) 
mean(repl) # 0.1977225

# replication success of null effect
replNULL <- (abs(z.repl) <= qnorm(1 - 0.05/2)) # combined success rate from elife was 46% (51/112): https://elifesciences.org/articles/71601
mean(c(repl,replNULL)) # 0.3963308


# simulate millions replicates
set.seed(2023)
SNR <- rmix(10^5, p, mu, tau)
Z <- rnorm(10^5, SNR, 1)
Z.repl <- SNR + rnorm(10^5)
REPL <- (Z * Z.repl > 0 ) & (abs(Z.repl) > 1.96)
mean(REPL) # 0.38592 is quite different from above where value was 0.1977225


# model the relationshio between z value and fit_repl_z
fit_repl_z <- glm(REPL ~ splines::ns(abs(Z),4), family = "binomial")

# make fig
## prepare dataframe for visualization - get predicted replication success
preds <- predict(fit_repl_z, type = "link", se.fit = TRUE) %>% data.frame()

## back-transform - in essence it is inverse logit transformation (plogis())
preds <- preds %>% mutate(ll.fit = fit - qnorm(1-0.05/2) * se.fit,
                          ul.fit = fit + qnorm(1-0.05/2) * se.fit) %>% 
  mutate(est = fit_repl_snr$family$linkinv(fit), # back-transform
         ll = fit_repl_snr$family$linkinv(ll.fit),
         ul = fit_repl_snr$family$linkinv(ul.fit))

# predict(fit_repl_year, type="response") # response scale

#png(filename = "./fig 1C - z.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
Fig_1C2 <- preds %>% ggplot(aes(x = abs(Z), y = est)) + 
  #geom_point(shape = 21, fill = 'grey95') + 
  geom_line() + 
  geom_segment(aes(x = quantile(abs(Z), 0.25), y=0, xend=quantile(abs(Z), 0.25), yend=0.10), arrow = arrow(length=unit(0.2, 'cm'), ends="last", type = "closed"), color="lightgrey", lwd=0.5) + 
  annotate("text", x = quantile(abs(Z), 0.25), y = 0.10/2, label = "Q25", color = brewer.pal(n = 8, name = "Dark2")[6]) +
  geom_segment(aes(x = quantile(abs(Z), 0.50), y=0, xend=quantile(abs(Z), 0.50), yend=0.22), arrow = arrow(length=unit(0.2, 'cm'), ends="last", type = "closed"), color="lightgrey", lwd=0.5) + 
  annotate("text", x = quantile(abs(Z), 0.50), y = 0.22/2, label = "Q50", color = brewer.pal(n = 8, name = "Dark2")[6]) + 
    geom_segment(aes(x = quantile(abs(Z), 0.75), y=0, xend=quantile(abs(Z), 0.75), yend=0.62), arrow = arrow(length=unit(0.2, 'cm'), ends="last", type = "closed"), color="lightgrey", lwd=0.5) + 
  annotate("text", x = quantile(abs(Z), 0.75), y = 0.62/2, label = "Q75", color = brewer.pal(n = 8, name = "Dark2")[6]) + 
  geom_line(mapping=aes(x = abs(Z), y = ll), linetype = "dashed", col = "red") + 
  geom_line(mapping=aes(x = abs(Z), y = ul), linetype = "dashed", col = "red") + 
  labs(x = "Test statistic of single experiment (z value)", y = 'Replication success') + 
  scale_x_continuous(labels = scales::number_format(accuracy = 1), 
                     limits = c(0,10), breaks = seq(0,10,2)
                     ) + # need "scales" package
  
  guides(fill = "none", colour = "none") + 
  theme_cowplot() + 
  theme(legend.position = c(0, 1), legend.justification = c(0, 1)) + 
  theme(legend.direction = "horizontal") + 
  theme(legend.background = element_blank()) + 
  theme(axis.title = element_text(size = 14, colour = "black"),
        axis.text.x =  element_text(size = 14, colour = "black"),
        axis.text.y =  element_text(size = 14, colour = "black"),
        panel.grid = element_blank()) 
Fig_1C2
#dev.off()

```

# Fig 1D - replication vs. year

Predictor analysis 1: we want to examine whether the replication success is decreasing over time - decline effect or time-lag bias in ecology and evolution.

In many fields, early papers reported a very large effect size, which is hard to replicate.

From the modelling perspective, we need to model use replication success as response variable and publication year as independent variable (a predictor/covariate).

```{r}
# get publicaiton year from our dataset
year <- dat$study.year

# fit glm model with replication success as y and publication year as x.
fit_repl_year <- glm(repl ~ splines::ns(year,2), family = "binomial")
coef(summary(fit_repl_year)) # plogis(coef(summary(fit_repl_year)))

# prepare dataset for visualize the relationship between replication success and year
preds <- predict(fit_repl_year, type = "link", se.fit = TRUE) %>% data.frame()

preds <- preds %>% mutate(ll.fit = fit - qnorm(1-0.05/2) * se.fit,
                          ul.fit = fit + qnorm(1-0.05/2) * se.fit) %>% 
  mutate(est = fit_repl_year$family$linkinv(fit), # back-transform
         ll = fit_repl_year$family$linkinv(ll.fit),
         ul = fit_repl_year$family$linkinv(ul.fit)) %>%
  mutate(year = year,
         snr = snr,
         study = dat$study)

#png(filename = "./fig 1D - year.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
Fig_1D <-  preds %>% ggplot(aes(x = year, y = est)) + 
  geom_line() + 
  geom_ribbon(aes(x = year, ymax=ul, ymin=ll), fill="red", alpha=0.25) + 
  labs(x = "Publication year", y = 'Replication success [\u00B1 CI]') + 
  scale_x_continuous(#labels = scales::number_format(accuracy = 1), 
                     limits = c(1930,2020), breaks = seq(1930,2020,15)) + # need "scales" package
  #ylim(0.15,0.35) + 
  #guides(fill = "none", colour = "none") + 
  #theme_bw() +
  theme_cowplot() + 
  theme(axis.title =  element_text(size = 14, colour = "black"),
        axis.text =  element_text(size = 14, colour = "black"))
Fig_1D
#dev.off()



```


# Fig 1E - replication success vs. effect size measure

Predictor analysis 2: we want to examine whether effect size measure is a predictor of replication success - whether effect size measure causes systematically difference in replication success.

In our dataset, we have 4 types of effect size measures:
(1) SMD - standardized mean difference
A typical additive measure of effect of interest

(2) lnRR - ratio of mean of two arms (on the log scale)
A typical multiplicative measure of effect of interest

(3) Zr - Fish's transformation of correlation coefficient
A typical correlation measure of effect of intetest

(4) uncommon
Uncommon effect size measures such as log odds ratio, relative risk ratio, regression coefficient (slope)


From the modelling perspective, we need to model use replication success as response variable and effect size measure as independent variable (a predictor/covariate).

```{r}
# get effect size measure from our dataset
grouped_es <- as.factor(dat$grouped_es)
grouped_es <- factor(grouped_es, levels = c("SMD","lnRR","Zr","uncommon"))
# fit glm with replication success as y and effect size measure as x
fit_repl_es <- glm(repl ~ grouped_es - 1, family = "binomial")
plogis(coef(summary(fit_repl_es)))

# prepare dataset for visualize the relationship between replication success and effect size measure
ci <- confint(fit_repl_es) # get confidence interval
df_est <- data.frame(group = c("Additive","Multiplicative","Correlation","Uncommon"), # c("SMD","lnRR","Zr","uncommon")
                     est = plogis(coef(summary(fit_repl_es)))[,1],
                     se = plogis(coef(summary(fit_repl_es)))[,2],
                     ll = plogis(ci)[,1],
                     ul = plogis(ci)[,2])
df_est$group <- as.factor(df_est$group)
df_est$group <- factor(df_est$group, levels = c("Uncommon", "Correlation", "Multiplicative", "Additive")) # c("uncommon", "Zr", "lnRR", "SMD")

#png(filename = "./fig 1E - es.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
Fig_1E <- df_est %>% ggplot(aes(x = group, y = est, ymin = ll, ymax = ul, colour = group)) +
    geom_pointrange(size = 1, show.legend = F, fatten = 10, position = position_nudge(x = 0)) +
    scale_y_continuous(labels = scales::number_format(accuracy = 0.01), limits = c(0.175,0.21)) + 
  coord_flip() +
  labs(y = "Replication success [\u00B1 CI]", x = "Effect size measure") +
  theme_cowplot() + 
  scale_color_manual(values = wes_palette("BottleRocket2", 4, type = "discrete")) + 
  #viridis::scale_color_viridis(discrete = T, option = "viridis")
   theme(axis.title = element_text(size = 14, colour = "black"),
         axis.text.x =  element_text(size = 14, colour = "black"),
         axis.text.y =  element_text(size = 14, colour = "black"),
         legend.text = element_text(size = 14, colour = "black"),
        #legend.title = element_text(size = 14, colour = "black"),
        panel.grid = element_blank(),
        #axis.ticks = element_line(size = 1, colour = "black"),
        axis.ticks.length = unit(0.15, "cm")
        #panel.border = element_rect(size = 1.2, colour = "black", fill = NA)
        ) 
Fig_1E 
#dev.off()
```


# Fig 1F - replication success vs. approach

Predictor analysis 3: we want to examine whether study approach (experiment vs. observation) is a predictor of replication success

We use a subset where we have study approach information for primary studies included in meta-analyses. 
Generally, primary studies have two approaches:
Experiment/manipulation and observation

From the modelling perspective, we need to model use replication success as response variable and study approach as independent variable (a predictor/covariate).

```{r}
# load the subset
dat_appro_csv <- list.files(here("data/approach_dat"), pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)  ## need to use full name of each dataset, otherwise read_csv is not able to read it
csv_filenames <- list.files(here("data/approach_dat"), pattern = "*.csv", full.names = FALSE) # get names of each .csv file
names(dat_appro_csv) <- csv_filenames # rename the elements of the list

# only select useful variables
dat_appro_list <- NA
for (i in 1:length(dat_appro_csv)) {
  dat_appro_list[i] <- dplyr::select(dat_appro_csv[[i]], yi, vi, Study) %>% list()
}
# unlist
dat_appro <- plyr::ldply(dat_appro_list, data.frame)
# calculate z value
dat_appro <- dat_appro %>% mutate(se = sqrt(vi),
                                  z2 = yi / se)

z2 = dat_appro$z2
# fit mixture model
fit2 <- flexmix(z2 ~ 0, k = 3, data = dat_appro) # non-zero distribution with 4 components led to less than 1 sigma
p2 <- summary(fit2)@comptab$prior
mu2 <- rep(0, length(p2))
sigma2 <- modeltools::parameters(fit2)
ind2 <- order(sigma2)
p2 <- p2[ind2]
mu2 <- mu2[ind2]
sigma2 <- sigma2[ind2]
tau2 <- sqrt(sigma2^2 - 1)

# visualize distribution
dist <- data.frame(x, y = drop(dmix(x,p2,mu2,sigma2)))
#df <- data.frame(z = z)
df <- data.frame(z2 = c(-z2, z2))

df %>% 
  ggplot(aes(x = z2)) + 
  geom_histogram(aes(y = ..density..), 
                 #breaks = breaks, 
                 fill = brewer.pal(n = 8, name = "Dark2")[1], 
                 color = brewer.pal(n = 8, name = "Dark2")[8],
                 alpha = .9,
                 bins = 20) +
  theme_cowplot() + 
  labs(x = "Test statsitic of signal experiment (z value)", y = "Density", title = "Subset") + 
  scale_fill_brewer(palette="Dark2") +
  xlim(-10,10) +
  geom_line(data = dist, aes(x = x, y = y), color = brewer.pal(n = 8, name = "Dark2")[2], alpha=1, linewidth = 1.5)



# simulate replicates
snr2 <- rmix(length(z2), p2, mu2, tau2)
z.repl2 <- snr2 + rnorm(length(z2))
repl2 <- (z2 * z.repl2 > 0 ) & (abs(z.repl2) > qnorm(1 - 0.05/2))
# mean replication success
mean(repl2) # 0.2625422

# retrieve study approach information
approach <- c(rep("Exp",nrow(dat_appro_csv[[1]])),
              rep("Exp",nrow(dat_appro_csv[[2]])),
              rep("Exp",nrow(dat_appro_csv[[3]])),
              rep("Exp",nrow(dat_appro_csv[[4]])),
              rep("Exp",nrow(dat_appro_csv[[5]])),
              rep("Exp",nrow(dat_appro_csv[[6]])),
              rep("Exp",nrow(dat_appro_csv[[7]])),
              rep("Obs",nrow(dat_appro_csv[[8]])),
              rep("Obs",nrow(dat_appro_csv[[9]])),
              rep("Obs",nrow(dat_appro_csv[[10]])),
              rep("Obs",nrow(dat_appro_csv[[11]])),
              rep("Obs",nrow(dat_appro_csv[[12]])),
              rep("Obs",nrow(dat_appro_csv[[13]])),
              rep("Exp",nrow(dat_appro_csv[[14]])),
              rep("Exp",nrow(dat_appro_csv[[15]])),
              rep("Obs",nrow(dat_appro_csv[[16]])),
              rep("Obs",nrow(dat_appro_csv[[17]])),
              rep("Obs",nrow(dat_appro_csv[[18]])),
              rep("Obs",nrow(dat_appro_csv[[19]])), # mix was assumed to be obs
              rep("Obs",nrow(dat_appro_csv[[20]])),
              rep("Obs",nrow(dat_appro_csv[[21]])),
              rep("Exp",nrow(dat_appro_csv[[22]])),
              rep("Exp",nrow(dat_appro_csv[[23]])),
              rep("Exp",nrow(dat_appro_csv[[24]])),
              rep("Exp",nrow(dat_appro_csv[[25]])),
              rep("Exp",nrow(dat_appro_csv[[26]])),
              rep("Exp",nrow(dat_appro_csv[[27]])),
              rep("Exp",nrow(dat_appro_csv[[28]])),
              rep("Obs",nrow(dat_appro_csv[[29]])), # mix was assumed to be obs
              rep("Obs",nrow(dat_appro_csv[[30]]))) # info from Hillebrand H, Donohue I, Harpole W S, et al. Thresholds for ecological responses to global change do not emerge from empirical data[J]. Nature Ecology & Evolution, 2020, 4(11): 1502-1509.

# fit glm with replication success as y and study approach as x
fit_repl_appro <- glm(repl2 ~ approach - 1, family = "binomial")
plogis(coef(summary(fit_repl_appro)))

# prepare dataframe for visualizing the relationship between replicaiton success and study appraoch
ci <- confint(fit_repl_appro)
df_est <- data.frame(group = c("Exp","Obs"),
                     est = plogis(coef(summary(fit_repl_appro)))[,1],
                     se = plogis(coef(summary(fit_repl_appro)))[,2],
                     ll = plogis(ci)[,1],
                     ul = plogis(ci)[,2])
df_est$group <- as.factor(df_est$group)
df_est$group <- factor(df_est$group, levels = c("Obs", "Exp"))

#png(filename = "./fig 1F - approach.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
Fig_1F <- df_est %>% ggplot(aes(x = group, y = est, ymin = ll, ymax = ul, colour = group)) +
    geom_pointrange(size = 1, show.legend = F, fatten = 10, position = position_nudge(x = 0)) +
    scale_y_continuous(labels = scales::number_format(accuracy = 0.01), limits = c(0.20,0.30)) + 
  coord_flip() +
  labs(y = "Replication success [\u00B1 CI]", x = "Study approach") +
  theme_cowplot() + 
  scale_color_manual(values = wes_palette("GrandBudapest2", 4, type = "discrete")) + 
  #viridis::scale_color_viridis(discrete = T, option = "viridis")
   theme(axis.title = element_text(size = 14, colour = "black"),
        axis.text.x =  element_text(size = 14, colour = "black"),
        axis.text.y =  element_text(size = 14, colour = "black"),
        legend.text = element_text(size = 14, colour = "black"),
        #legend.title = element_text(size = 14, colour = "black"),
        panel.grid = element_blank(),
        #axis.ticks = element_line(size = 1, colour = "black"),
        axis.ticks.length = unit(0.15, "cm")
        #panel.border = element_rect(size = 1.2, colour = "black", fill = NA)
        ) 
Fig_1F 
dev.off()

```

# Fig 1
```{r}
png(filename = "./Fig 1.png", width = 10, height = 10, units = "in", res = 400, type = "windows")
plot_grid(Fig_1A,                           
          Fig_1B,
          Fig_1C2,
          Fig_1D,
          Fig_1E,
          Fig_1F,
          ncol = 2,
          nrow = 3,
          hjust = 0,
          #labels = c("A", "ggplot2"), 
          labels = "AUTO"
          ) %>% suppressWarnings()
dev.off() 
```


# Fig 2A - Power

```{r}
# conventional power
power <- pnorm(snr - qnorm(1-0.05/2)) + 1 - pnorm(snr + qnorm(1-0.05/2))
quantile(power,c(0.1,0.25,0.5,0.75,0.9),na.rm = T)
# actual power
# power.actu <- 1 - pnorm(qnorm(1-0.05/2),abs(z),1)


# prepare dataset for fig
Manhattan_dat <- data.frame(study = dat$study, # retrieve study ID from dataset
                            power = power,
                            grouped_es = as.factor(dat$grouped_es)) # retrieve effect size measure from dataset

# violin
#png(filename = "./fig 2A2 - power.jpg", width = 5, height = 5, units = "in", res = 400, type = "windows")
Fig_2A2 <- ggviolin(Manhattan_dat, "grouped_es", "power", fill = "grouped_es",
   palette = wes_palette("BottleRocket2", 4, type = "discrete"), 
   add = "boxplot", add.params = list(fill = "white")) + 
  guides(fill = "none") + 
  scale_x_discrete(labels = c("Additive","Multiplicative","Correlation","Uncommon"))+
  labs(x = "Effect size measure", y = "Statistical power")
Fig_2A2
#dev.off()


```


# Fig 2B - M

```{r}
# z.repl2 = rnorm(n, snr, 1)
M <- abs(z.repl/snr)
M.error <- M[abs(z.repl) > qnorm(1-0.05/2)]
summary(M.error)
quantile(M, probs = c(0.1, 0.25, 0.5, 0.75, 0.9), na.rm = T)

# the percent of different degree of exaggeration
M_deflation <- length(M.error[M.error < 1]) / length(M.error)
M_0_100 <- length(M.error[M.error >= 1 & M.error <= 2]) / length(M.error)
M_100_300 <- length(M.error[M.error > 3 & M.error <= 4]) / length(M.error)
M_300_inf <- length(M.error[M.error > 4]) / length(M.error)

# get percent
M_deflation <- length(M[M < 1]) / length(M)
M_0_100 <-  length(M[M >= 1 & M <= 2]) / length(M)
M_100_300 <-  length(M[M > 3 & M <= 4]) / length(M)
M_300_inf <-  length(M[M > 4]) / length(M)


#library(quantreg)
#fit.M0.5 <- rq(M ~ splines::ns(abs(z.repl), 4), tau = 0.5, method = "fn")
#summary(fit.M0.5)


## prepare dataframe for figure
Manhattan_dat <- data.frame(study = dat$study, # retrieve study ID from dataset
                            M = M,
                            grouped_es = as.factor(dat$grouped_es)) # retrieve effect size measure from dataset

# violin
#png(filename = "./fig 2B2 - M.jpg", width = 5, height = 5, units = "in", res = 400, type = "windows")
Fig_2B2 <- ggviolin(subset(Manhattan_dat2, M2 < 15), "grouped_es", "M2", fill = "grouped_es", size = 0.00000000000001, 
   palette = wes_palette("Rushmore1", 4, type = "discrete"),
   add = "boxplot", add.params = list(fill = "white")) +
  guides(fill = "none") + 
  scale_x_discrete(labels = c("Additive","Multiplicative","Correlation","Uncommon"))+
  labs(x = "Effect size measure", y = "Exaggeration ratio")
Fig_2B2
#dev.off()

```


# Fig 2C - S

```{r}
S <- (z.repl * snr) < 0
mean(S[abs(z.repl) > qnorm(1-0.05/2)])


# fit glm with z value as response variable
fit_S_z.repl <- glm(S ~ splines::ns(abs(z.repl),4), family = "binomial")

# get predicted replication success for each data point in our dataset
preds <- predict(fit_S_z.repl, type = "link", se.fit = TRUE) %>% data.frame()


## prepare df for fig
Manhattan_dat <- data.frame(study = dat$study, # retrieve study ID for each replication success datapoint
                            S = fit_S_z.repl$family$linkinv(preds$fit),
                            grouped_es = as.factor(dat$grouped_es)) # retrieve effect size measure


# violin
#png(filename = "./fig 2C2 - S.jpg", width = 5, height = 5, units = "in", res = 400, type = "windows")
Fig_2C2 <- ggviolin(Manhattan_dat, "grouped_es", "S", fill = "grouped_es", 
   palette = wes_palette("Moonrise3", 4, type = "discrete"),
   add = "boxplot", add.params = list(fill = "white")) + 
  guides(fill = "none") + 
  scale_x_discrete(labels = c("Additive","Multiplicative","Correlation","Uncommon"))+
  labs(x = "Effect size measure", y = "Sign error")
Fig_2C2
#dev.off()

```


# Fig shrinkage

```{r}
# calculate beta hat directly using help function
b_hat <- NA
for (i in 1:nrow(dat)) {
  b_hat[i] <- posterior_bhat(dat$eff.size[i],dat$se.eff.size[i],p,tau) %>% list() 
}
b_hat <- unlist(b_hat)

# calculate expectation of snr first, and rescale it to obtain beta hat
post_snr <- NA
for (i in 1:nrow(dat)) {
  post_snr[i] <- posterior_snr(dat$eff.size[i],dat$se.eff.size[i],p,tau) %>% list()
}

b_hat2 <- NA
for (i in 1:nrow(dat)) {
  b_hat2[i] <- ( dat$se.eff.size[i] * sum(post_snr[[i]]$q * post_snr[[i]]$pm) ) %>% list()
}
b_hat2 <- unlist(b_hat2)

# two ways of estimate b hat are the same
all.equal(b_hat,b_hat2)


# calculate shrink factor lamda
#lamda = sapply(dat, function(x) shrink(b=x$eff.size,s=x$se.eff.size,p=p,tau=tau))
lamda <- NA
for (i in 1:nrow(dat)) {
  lamda[i] <- ( shrink(b=dat$eff.size[i],s=dat$se.eff.size[i],p=p,tau=tau) ) %>% list()
}
lamda <- unlist(lamda)


dat_shrink <- data.frame(study = dat$study, b = dat$eff.size, b_hat = b_hat, lamda = lamda, s = dat$se.eff.size, z = dat$z, grouped_es = dat$grouped_es)
dat_shrink <- dat_shrink %>% filter(b < 15 & b > -15) 
#png(filename = "./ b - bhat.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
#dat_shrink %>% filter(b < 20 & b > -20) %>% ggplot(aes(x = abs(b), y = abs(b_hat))) + 
  #geom_point(shape = 21, fill = 'grey95') + 
#  geom_point() + 
#  labs(x = "Naive estimate", y = 'shrinkage estimate') + 
#  guides(fill = "none", colour = "none") + 
  #theme_bw() +
#  theme_cowplot() + 
#  theme(legend.position = c(0, 1), legend.justification = c(0, 1)) + 
#  theme(legend.direction = "horizontal") + 
#  theme(legend.background = element_blank()) + 
#  theme(axis.title = element_text(size = 10, colour = "black"),
#        axis.text.x =  element_text(size = 10, colour = "black"),
#        axis.text.y =  element_text(size = 10, colour = "black"),
#        legend.text = element_text(size = 10, colour = "black"),
        #legend.title = element_text(size = 14, colour = "black"),
#        panel.grid = element_blank(),
        #axis.ticks = element_line(size = 1, colour = "black"),
#        axis.ticks.length = unit(0.15, "cm")
        #panel.border = element_rect(size = 1.2, colour = "black", fill = NA)
#        ) 
#dev.off()

# aggregate at study level
dat_shrink_agg <- dat_shrink %>% dplyr::group_by(study) %>%
  summarise_at(.vars = vars(b,b_hat), .funs = mean)

dat_shrink_agg <- dat_shrink_agg %>% 
  mutate(across(.cols = "study",~ map_chr(.x, ~ gsub("\"", "", .x)))) # remove string - https://stackoverflow.com/questions/21338114/remove-quotes-from-a-data-frame-in-r

names(dat_shrink_agg) <- c("study", "b2", "b_hat2")

dat_shrink_uniq <- distinct(dat_shrink, study, .keep_all = TRUE)
dat_shrink2 <- left_join(dat_shrink_uniq, dat_shrink_agg, by = "study") 


# Raincloud 1 x 1 repeated measures
df_1x1 <- data_1x1(
  array_1 = abs(dat_shrink2$b2),
  array_2 = abs(dat_shrink2$b_hat2),
  jit_distance = .09,
  jit_seed = 321)

#png(filename = "paired_b_VS_bhat.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
Fig_2D <- raincloud_1x1_repmes(
  data = df_1x1,
  colors = (c('dodgerblue', 'darkorange')),
  fills = (c('dodgerblue', 'darkorange')),
  line_color = 'gray',
  line_alpha = .3,
  size = 1,
  alpha = .6,
  align_clouds = FALSE) +
 
scale_x_continuous(breaks=c(1,2), labels=c("Naive estimator", "Shrinkage estimator"), limits=c(0, 3)) +
  xlab("Estimator") + 
  ylab(expression(paste("Effect size estimate"))) +
  theme_classic()

Fig_2D
#dev.off()

```


# Fig 2
```{r}

png(filename = "./Fig 2s.png", width = 10, height = 10, units = "in", res = 400, type = "windows")
plot_grid(Fig_2A2,                           
          Fig_2B2,
          Fig_2C2,
          Fig_2D,
          ncol = 2,
          nrow = 2,
          labels = c("A", "B", "C", "D") 
          ) %>% suppressWarnings()
dev.off() 


```